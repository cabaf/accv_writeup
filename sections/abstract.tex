This paper describes a framework for modeling human actions incorporating a new set of visual cues that represent the action's \emph{surrounding}. \B{I kept the word surrounding here, but I think we should replace it with an other term such as context} We develop a weak foreground-background segmentation approach in order to robustly extract not only human aligned features, but also global motion and context information. Using dense point trajectories, our approach separates and describes the foreground motion from the background, represents the appearance of the extracted static background, and encodes the global camera motion that interestingly is shown to be discriminative for certain action classes. Our experiments on four challenging benchmarks (HMDB51, Hollywood2, Olympic Sports, UCF50) show that our surrounding features enable a significant performance improvement over state-of-the-art algorithms.
