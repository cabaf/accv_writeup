\section{Introduction}
\label{introduction}

Human action recognition is a challenging task for computer vision algorithms due to the large variabilities in video data caused by occlusions,
camera motions, actor and scene appearances, among others. A popular current trend in action recognition methods relies on using local video
descriptors to represent visual events in videos \cite{Dollar2005, Laptev2005, WangCVPR2011}. These features are usually aggregated into
a compact representation, most commonly into a bag-of-features (BoF) representation framework \cite{Schuldt2004}. The advantage of this
simple representation is that it avoids difficult pre-processing steps such as motion segmentation and tracking.
In the BoF representation, local descriptors are quantized using a pre-computed codebook of visual patterns. This representation combined
with discriminative classifiers such as support vector machines (SVM), has achieved tremendous success in action recognition in controlled
scenarios \cite{Blank2005,Schuldt2004}. Due to its simplicity, BoF requires the use of strong, robust and informative features, which can be 
obtained reliably in such simplified scenarios. However, recent efforts in the collection of more realistic datasets from movies and web site 
\cite{Kuehne2011,Marszalek2009,Rodriguez2008} represent a challenge for existing methods due to dynamic backgrounds, changes in light 
conditions and camera motions among other noisy conditions. 

\subsection{Related work}
A large amount of work has studied the problem of human action recognition in videos \cite{Aggarwal2011}. In
this section we overview some of the most relevant previous work on the topics of video stabilization, video
feature extraction and action recognition in videos.

A common methodology for video stabilization relies on estimating the global camera motion. One approach for
this estimation computes sparse visual features \cite{Brown2003,Capel1998, Zoghlami1997} such as corners
\cite{Censi1999} and estimates a warping matrix between consecutive frames. Others prefer to use all pixels
in the image to compute an alignment \cite{Lucas1981, Matsushita2006}, but tend to suffer under-fitting due
to local outliers. An alternative methodology defines a model for camera motion
\cite{Buehler2001,Hansen1994} and uses multiple frames to estimate its parameters. Unfortunately, there is a
large variation in camera motions and it is difficult to capture them in a single model. Once the camera
motion is estimated, most algorithms use it to perform image alignment or warping \cite{szeliski2006image}.
Unfortunately, warping usually introduces empty image regions in the aligned image. These areas may be
recovered using impainting methods \cite{Wexler2004} at a high computational cost. Finally, instead of fully
stabilizing sequences, \cite{Gleicher2008,GrundmannKwatra2011} propose to simulate professional camera
motion in videos taken with handheld cameras. Unfortunately, not all camera motion is removed and the
application of these methods to action recognition is limited. Most related to our approach, Park \etal
\cite{Park2013} recently show how the use of weak video stabilization based on a coarse optical flow can
lead to improved pedestrian detection in videos. Their goal is to isolate limb motion while cancelling
pedestrian translation and camera motion. In this paper, we explore the extension of this technique and its
applicability to feature extraction for action recognition.
In another line of work, researchers have studied the issue of extracting video features for recognition
that are robust to camera motion \cite{Jain2013,Gross2012,Wu2011a}. When applied to videos with large camera
movement, traditional video feature extraction methods tend to generate a large number of features that are
mostly related to the camera motion \cite{Dollar2005, Laptev2005, WangCVPR2011}. In order to overcome this
issue, Wu \etal \cite{Wu2011a} propose the use of Lagrangian particle trajectories for action description in
videos acquired with moving cameras. Their method compensates for the global camera motion and only extracts
features that exhibit motion independent to the camera movement, outperforming traditional feature
extraction algorithms. Matikainen \etal \cite{Matikainen2009} present a technique for action recognition
with quantized trajectories of tracked features. More recently, Wang \etal presents a method for action
recognition using dense sampling of point trajectories \cite{WangCVPR2011}. Their method handles large
camera motions by limiting the maximum length of tracked trajectories. In spite of their simplicity, these
dense trajectory features achieved state-of-the-art performance in bench-marking datasets. In order to
improve upon these dense trajectories, Jain \etal \cite{Jain2013} propose a method to estimate more reliable
motion features for action recognition. Their method obtains improvements on feature robustness  by first
decomposing optical flow into dominant and residual motions. Dominant motion is estimated using an affine
model and subtracted from the computed optical flow to obtain the residual motion. This information is then
used to compute local motion descriptors. While the method is simple and improves recognition performance,
residual and dominant motion estimations are not reliable when the dominant motion is related to the actor.