This paper describes a framework for modeling human actions incorporating surroundings cues. We consider a weak foreground-background segmentation approach in order to robustly extract not only human aligned features, but also global motion and context information. Our approach relies on the recently proposed Improved Trajectories to both separate and describe the foreground motion while computing SIFT and global motion on background feature points. Our experiments on four challenging benchmarks (HMDB51, Hollywood2, Olympic Sports, UCF50) show that our surrounding features provide significant performance improvements compared to state-of-the-art algorithms. 